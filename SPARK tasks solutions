1st task is to extract the data from csv files from hdfs, apply filters and ordering with specified columns.

2nd task is similar as 1st but to retrieve the data from json.

3rd and 4th tasks are on querying the hive tables, apply aggregate functionalities, conditions and ordering.


5th task is to use accumulator to perform word count.

Count the occurence the word 'are' in the given file


val newfile = sc.textFile("/user/stgvenu9/json_files/wc.txt")
val a = newfile.flatMap(rec => rec.split(" "))
val b = a.map(rec => (rec,1))
val areWc = sc.accumulator(0,"'are' count")
val c = b.filter(rec => { 
if (rec._1 == "are")
(areWc += 1)
rec._1 == "are"
})


If the file is comma separated

val url = "hdfs://nn01.itversity.com:8020/user/stgvenu9/mysql_tables"
val outputpath = "hdfs://nn01.itversity.com:8020/user/stgvenu9/spark"
val ordersRDD = sc.textFile(url + "/orders")
val orderitemsRDD = sc.textFile(url + "/order_items")

val ordersFilterInvokedAccum = sc.accumulator(0,"orders filter invoked count")
val ordersCompletedAccum = sc.accumulator(0,"orders completed count")
val ordersRDDcompleted = ordersRDD.filter(rec => { 
ordersFilterInvokedAccum += 1
if ( rec.split(',')(3).equals("COMPLETE"))
( ordersCompletedAccum  += 1)
rec.split(',')(3).equals("COMPLETE")
})


6th task is to submit a spark job on yarn.

7th task is to extract 2 data-sets which contains common column. Apply filters and join the data-sets to generate the required output.




